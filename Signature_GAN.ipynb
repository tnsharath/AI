{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Signature_GAN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "10sVc_EbmyqIT8BAm65wZUOFBL5qjCwpP",
      "authorship_tag": "ABX9TyNdMAOKL3347fguw+6OMnK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tnsharath/AI/blob/master/Signature_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnT5tG2rkuXU",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4k8oGdkzfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a45e6f06-9aa9-4902-e8ea-cc5e8e8271bc"
      },
      "source": [
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqBm1seyOqs",
        "colab_type": "text"
      },
      "source": [
        "## Get data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6wEuFcCniBd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "255f2f10-05e5-407f-f599-1962c96e3f2c"
      },
      "source": [
        "shutil.copy(\"/content/drive/My Drive/colab_work/signature_data.zip\", \"/content/\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/signature_data.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSCybzYPnySn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip /content/signature_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxLdhvEJyY6a",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3N2BHYsg70i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_data():\n",
        "  train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,)\n",
        "  training_set = train_datagen.flow_from_directory('data/train_set',\n",
        "                                                 target_size = (128, 128),\n",
        "                                                 batch_size = 258)\n",
        "  X_train, y_train = training_set.next()\n",
        "  #X_train = X_train.reshape(258, 128 * 128 * 3)\n",
        "  \n",
        "  return X_train, y_train"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNlH31-Wy1-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Dim\n",
        "np.random.seed(10)\n",
        "random_dim = 100"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTkRG7yLpDaR",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hl7eT9MeSLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(128 * 4 * 4 , input_dim=random_dim))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Reshape((4, 4, 128)))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(256, 5, padding= 'same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128, 4, strides=2, padding='same'))\n",
        "  model.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(3, 7, activation='tanh', padding= 'same'))\n",
        "  # model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQu4yzWZesOQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "ac293be0-7d70-4f13-897d-18ea8c1ac25a"
      },
      "source": [
        "get_generator().summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 2048)              206848    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 4, 4, 256)         819456    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 8, 8, 128)         524416    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 128)     262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 128, 128, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 3)       18819     \n",
            "=================================================================\n",
            "Total params: 2,618,627\n",
            "Trainable params: 2,618,627\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0SNYtp7gHNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator():\n",
        "    cnn = tf.keras.Sequential()\n",
        "\n",
        "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size= 1, input_shape=[128, 128, 3]) )\n",
        "    cnn.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "    cnn.add(tf.keras.layers.Dropout(0.3))\n",
        "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3))\n",
        "    cnn.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "    cnn.add(tf.keras.layers.Dropout(0.3))\n",
        "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "    cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3))\n",
        "    cnn.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "    cnn.add(tf.keras.layers.Dropout(0.3))\n",
        "    cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "    cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    cnn.add(tf.keras.layers.Dense(units=256))\n",
        "    cnn.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "    cnn.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "\n",
        "    cnn.add(tf.keras.layers.Dense(units=128))\n",
        "    cnn.add(tf.keras.layers.LeakyReLU(0.2))\n",
        "    cnn.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "    cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "    cnn.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "    return cnn\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDmBiN0Mj3rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "589eb63a-25d3-40c4-be70-eaabfe066316"
      },
      "source": [
        "get_discriminator().summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 62, 62, 32)        9248      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,657,537\n",
            "Trainable params: 1,657,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtLSkLVipCsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_network(discriminator, random_dim, generator):\n",
        "    # We initially set trainable to False since we only want to train either the\n",
        "    # generator or discriminator at a time\n",
        "    discriminator.trainable = False\n",
        "    # gan input (noise) will be 100-dimensional vectors\n",
        "    gan_input = tf.keras.Input(shape=(random_dim,))\n",
        "    # the output of the generator (an image)\n",
        "    x = generator(gan_input)\n",
        "    # get the output of the discriminator (probability if the image is real or not)\n",
        "    gan_output = discriminator(x)\n",
        "    gan = tf.keras.Model(inputs=gan_input, outputs=gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5))\n",
        "    return gan"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YBjTvJNqwHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a wall of generated images\n",
        "def plot_generated_images(epoch, generator, examples=25, dim=(5, 5), figsize=(10, 10)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\n",
        "    generated_images = generator.predict(noise)\n",
        "    generated_images = generated_images.reshape(examples, 128, 128, 3)\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(generated_images.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(generated_images[i], interpolation='nearest')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/My Drive/colab_work/results/gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONAaGT4sq206",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs=1, batch_size=128):\n",
        "    # Get the training \n",
        "    x_train, y_train= setup_data()\n",
        "\n",
        "    # Split the training data into batches of size 128\n",
        "    batch_count = int(x_train.shape[0] / batch_size)\n",
        "\n",
        "    # Build our GAN netowrk\n",
        "    generator = get_generator()\n",
        "    discriminator = get_discriminator()\n",
        "    gan = get_gan_network(discriminator, random_dim, generator)\n",
        "\n",
        "    for e in range(1, epochs+1):\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "        for _ in range(batch_count):\n",
        "            # Get a random set of input noise and images\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
        "\n",
        "            generated_images = generator.predict(noise)\n",
        "\n",
        "            \n",
        "            # Labels for generated and real data\n",
        "            y_dis = np.zeros(2*batch_size)\n",
        "            # One-sided label smoothing\n",
        "            y_dis[:batch_size] = 0.9\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator.trainable = True\n",
        "            #discriminator.train_on_batch(image_batch.reshape(128, 128, 128, 3), y_dis[:batch_size])\n",
        "            discriminator.train_on_batch(image_batch, y_dis[:batch_size])\n",
        "            discriminator.train_on_batch(generated_images, y_dis[batch_size:])\n",
        "\n",
        "            # Train generator\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
        "            y_gen = np.ones(batch_size)\n",
        "            discriminator.trainable = False\n",
        "            gan.train_on_batch(noise, y_gen)\n",
        "\n",
        "        if e == 1 or e % 20 == 0:\n",
        "           plot_generated_images(e, generator)\n",
        "\n",
        "        if e % 200 == 0:\n",
        "          generator.save_weights(\"/content/drive/My Drive/colab_work/results/gen_weights.h5\")\n",
        "          discriminator.save_weights(\"/content/drive/My Drive/colab_work/results/discriminator_weights.h5\")\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmVPTxDxq4cT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    train(2000, 128)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}